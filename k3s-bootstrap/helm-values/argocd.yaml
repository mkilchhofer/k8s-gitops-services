createAggregateRoles: true

global:
  networkPolicy:
    create: true
  addPrometheusAnnotations: true

applicationSet:
  metrics:
    enabled: true

    serviceMonitor:
      enabled: false # deployed via argocd-extra

  resources:
    limits:
      cpu: 50m
      memory: 256Mi
    requests:
      cpu: 5m
      memory: 48Mi

controller:
  enableStatefulSet: true
  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    readOnlyRootFilesystem: true
    runAsNonRoot: true

  metrics:
    enabled: true
    service:
      annotations:
        prometheus.io/honor_labels: "true"

    serviceMonitor:
      enabled: false # deployed via argocd-extra

  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 800Mi

dex:
  enabled: false

notifications:
  extraArgs:
    - --argocd-repo-server-strict-tls
  metrics:
    enabled: true

    serviceMonitor:
      enabled: false # deployed via argocd-extra

  notifiers:
    service.telegram: |
      token: $telegram-token
  templates:
    template.app-deployed: |
      email:
        subject: New version of an application {{.app.metadata.name}} is up and running.
      message: |
        Application {{.app.metadata.name}} is now running new version of deployments manifests.
    template.app-health-degraded: |
      email:
        subject: Application {{.app.metadata.name}} has degraded.
      message: |
        Application {{.app.metadata.name}} has degraded.
        Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}.
    template.app-health-suspended: |
      email:
        subject: Application {{.app.metadata.name}} is 'Suspended'
      message: |
        Application {{.app.metadata.name}} is 'Suspended'.
    template.app-sync-failed: |
      email:
        subject: Failed to sync application {{.app.metadata.name}}.
      message: |
        The sync operation of application {{.app.metadata.name}} has failed at {{.app.status.operationState.finishedAt}} with the following error: {{.app.status.operationState.message}}
        Sync operation details are available at: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true .
    template.app-sync-running: |
      email:
        subject: Start syncing application {{.app.metadata.name}}.
      message: |
        The sync operation of application {{.app.metadata.name}} has started at {{.app.status.operationState.startedAt}}.
        Sync operation details are available at: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true .
    template.app-sync-status-unknown: |
      email:
        subject: Application {{.app.metadata.name}} sync status is 'Unknown'
      message: |
        Application {{.app.metadata.name}} sync is 'Unknown'.
        Application details: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}.
        {{if ne .serviceType "slack"}}
        {{range $c := .app.status.conditions}}
            * {{$c.message}}
        {{end}}
        {{end}}
    template.app-sync-succeeded: |
      email:
        subject: Application {{.app.metadata.name}} has been successfully synced.
      message: |
        Application {{.app.metadata.name}} has been successfully synced at {{.app.status.operationState.finishedAt}}.
        Sync operation details are available at: {{.context.argocdUrl}}/applications/{{.app.metadata.name}}?operation=true .
  triggers:
    trigger.on-deployed: |
      - description: Application is synced and healthy. Triggered once per commit.
        oncePer: app.status.sync.revision
        send:
        - app-deployed
        when: app.status.operationState.phase in ['Succeeded'] and app.status.health.status == 'Healthy'
    trigger.on-health-degraded: |
      - description: Application has degraded
        send:
        - app-health-degraded
        when: app.status.health.status == 'Degraded'
    trigger.on-health-suspended: |
      - description: Application status is 'Suspended'
        send:
        - app-health-suspended
        when: app.status.health.status == 'Suspended'
    trigger.on-sync-failed: |
      - description: Application syncing has failed
        send:
        - app-sync-failed
        when: app.status.operationState.phase in ['Error', 'Failed']
    trigger.on-sync-running: |
      - description: Application is being synced
        send:
        - app-sync-running
        when: app.status.operationState.phase in ['Running']
    trigger.on-sync-status-unknown: |
      - description: Application status is 'Unknown'
        send:
        - app-sync-status-unknown
        when: app.status.sync.status == 'Unknown'
    trigger.on-sync-succeeded: |
      - description: Application syncing has succeeded
        send:
        - app-sync-succeeded
        when: app.status.operationState.phase in ['Succeeded']
    defaultTriggers: |
      - on-sync-status-unknown
  resources:
    limits:
      cpu: 50m
      memory: 128Mi
    requests:
      cpu: 1m
      memory: 64Mi

redis:
  exporter:
    enabled: true
    image:
      repository: quay.io/oliver006/redis_exporter
      tag: v1.43.1
    resources:
      limits:
        cpu: 50m
        memory: 64Mi
      requests:
        cpu: 5m
        memory: 28Mi

  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    readOnlyRootFilesystem: true
    runAsNonRoot: true

  metrics:
    enabled: true

    serviceMonitor:
      enabled: false # deployed via argocd-extra

  resources:
    limits:
      cpu: 50m
      memory: 64Mi
    requests:
      cpu: 5m
      memory: 24Mi

repoServer:
  certificateSecret:
    enabled: true
  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    readOnlyRootFilesystem: true
    runAsNonRoot: true

  metrics:
    enabled: true

    serviceMonitor:
      enabled: false # deployed via argocd-extra

  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 15m
      memory: 128Mi

  volumes:
    - name: gpg-private-keys
      secret:
        secretName: gpg-private-keys
        optional: true
  volumeMounts:
    - mountPath: /gpg-private-keys/
      name: gpg-private-keys

server:
  extraArgs:
    []
  certificateSecret:
    enabled: true
  containerSecurityContext:
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
    readOnlyRootFilesystem: true
    runAsNonRoot: true
  ingress:
    enabled: false # deployed via argocd-extra
    annotations:
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      cert-manager.io/cluster-issuer: letsencrypt
      cert-manager.io/revision-history-limit: "1"
      nginx.ingress.kubernetes.io/proxy-read-timeout: 120
    hosts: ["argocd.tools.kilchhofer.info"]
    ingressClassName: nginx-internal
    tls:
      - secretName: argocd-ingress-cert
        hosts: ["argocd.tools.kilchhofer.info"]

  metrics:
    enabled: true

    serviceMonitor:
      enabled: false # deployed via argocd-extra

  resources:
    limits:
      cpu: 100m
      memory: 256Mi
    requests:
      cpu: 10m
      memory: 80Mi

configs:
  cm:
    url: https://argocd.tools.kilchhofer.info

    helm.valuesFileSchemes: http, https, secrets+gpg-import-kubernetes

    admin.enabled: "true" # Required to use argocd TF provider
    oidc.config: |
      name: Dex
      issuer: https://sso.cloud.kilchhofer.info
      clientID: argocd
      clientSecret: $oidc.dex.clientSecret
      requestedScopes:
      - openid
      - email
      - groups

    # Enable exec and RBAC for pod logs
    server.rbac.log.enforce.enable: "true"
    exec.enabled: "true"

    # Tell Argo CD to ignore kyverno reports on the UI
    resource.exclusions: |
      - apiGroups:
          - kyverno.io
        kinds:
          - AdmissionReport
          - BackgroundScanReport
          - ClusterAdmissionReport
          - ClusterBackgroundScanReport
        clusters:
          - '*'
      - apiGroups:
          - cilium.io
        kinds:
          - CiliumEndpoint
        clusters:
          - '*'

  # params:
  #   server.insecure: "true"

  rbac:
    policy.csv: |
      g, kilchhofer-home:admins, role:admin
    policy.default: role:readonly

  credentialTemplates:
    gh-mkilchhofer-ssh:
      url: git@github.com:mkilchhofer
    internal-mkilchhofer-ssh:
      url: ssh://git@192.168.93.88:3001/mkilchhofer

  ssh:
    extraHosts: |
      [192.168.93.88]:3001 ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCwfVmMvq4ZuLciMlkjVCyygOWkGCPPzcz72jsM/SK7KqFeGooSCQNazGkAXHC8tcYIy25ihuMB19kkfiVa1uFehOekuoBLAjnYXzIlDUG/WaUbyLXjQ51c6QwJ2l8KUjAv8ccyBsd7ufl5ckvJmMTFKVl3Wx8bz1MD4scGVMh1cJ2IRdcPWRmY56iZP2jQdzgYX0Wi/2nJ04JJ4xXXFRBJ+MObmrAB3AhNvbV0QDuCHghLo/z5N/ZmzJa+zEHZh8uOzXHFmo4PKblGX3bVuPR9oohS+l1QcdQi7bF+YMVcxOkP0z0TwTleoMqIOXxjckhPHJ48oM4N5OZldritypDP

extraObjects:
  - apiVersion: cilium.io/v2
    kind: CiliumNetworkPolicy
    metadata:
      name: "argocd-repo-server-egress"
      namespace: "{{ .Release.Namespace }}"
    spec:
      endpointSelector:
        matchLabels:
          app.kubernetes.io/component: repo-server
      egress:
        - toEntities:
            - "all"

  - apiVersion: cilium.io/v2
    kind: CiliumNetworkPolicy
    metadata:
      name: "argocd-server-egress"
      namespace: "{{ .Release.Namespace }}"
    spec:
      endpointSelector:
        matchLabels:
          app.kubernetes.io/component: server
      egress:
        - toFQDNs:
            - matchName: sso.cloud.kilchhofer.info
          toPorts:
            - ports:
                - port: "443"
